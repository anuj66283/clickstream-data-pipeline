version: '3.9'

services:

  generator:
    build:
      context: src
      dockerfile: Dockerfile
    volumes:
      - ./src:/code
    ports: 
      - "8000:8000"
    networks:
      - forapi

  broker:
    image: docker.io/bitnami/kafka:3.7
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9500:9500"
    volumes:
      - "kafka_data:/bitnami"
      - "./kafka/jmx_exporter_config.yml:/etc/jmx_exporter/config.yml"
      - "./kafka/javaagent.jar:/etc/jmx/javaagent.jar"
    env_file: .env
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@broker:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,INTERNAL://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://${BROKER},INTERNAL://broker:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - ALLOW_PLAINTEXT_LISTENER=yes
      - BITNAMI_DEBUG=yes
      - KAFKA_JMX_PORT=9404
      - KAFKA_OPTS=-javaagent:/etc/jmx/javaagent.jar=9500:/etc/jmx_exporter/config.yml
    
    healthcheck:
      test: ["CMD", "sh", "-c", "unset KAFKA_OPTS && kafka-broker-api-versions.sh --bootstrap-server localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - forapi

  kafka-custom:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    volumes:
      - ./kafka:/code
    env_file: .env
    depends_on: 
      broker:
        condition: service_healthy
    command: python3 main.py
    networks:
      - forapi

  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    volumes:
      - ./spark:/opt/bitnami/spark/jobs/
      - ./spark/db:/opt/bitnami/spark/tmp
    command: bin/spark-class org.apache.spark.deploy.master.Master
    env_file: .env
    ports:
      - "8080:8080"
      - "7077:7077"
    depends_on:
      - kafka-custom
    networks:
      - forapi  

  spark-worker-1:
    build:
      context: ./spark
      dockerfile: Dockerfile
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    env_file: .env
    volumes:
      - ./spark/db:/opt/bitnami/spark/tmp
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=Worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1g
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - forapi

  spark-worker-2:
    build:
      context: ./spark
      dockerfile: Dockerfile
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    env_file: .env
    volumes:
      - ./spark/db:/opt/bitnami/spark/tmp
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=Worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1g
      - SPARK_MASTER_URL=spark://spark-master:7077
    networks:
      - forapi
  
  prometheus:
    image: prom/prometheus
    container_name: prometheus-mon
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yaml:/etc/prometheus/prometheus.yml
    networks:
      - forapi


volumes:
  kafka_data:
    driver: local

networks:
  forapi:
    driver: bridge